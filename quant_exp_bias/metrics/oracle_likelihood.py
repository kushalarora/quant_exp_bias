from overrides import overrides
from typing import List

from allennlp.training.metrics.metric import Metric

from lmpl.oracles.oracle_base import Oracle

import logging
import numpy as np
import math
import torch

logger = logging.getLogger(__name__)  # pylint: disable=invalid-name

@Metric.register("oracle_likelihood")
class OracleLikelihood(Metric):
    """
    This :class:`Metric` breaks with the typical ``Metric`` API and just stores values that were
    computed in some fashion outside of a ``Metric``.  If you have some external code that computes
    the metric for you, for instance, you can use this to report the average result using our
    ``Metric`` API.
    """

    def __init__(self,
                 oracle: Oracle,
                 add_brevity_penalty: bool = False,
                 log_cost: bool = False,
                ) -> None:
        self._oracle = oracle
        self._add_brevity_penalty = add_brevity_penalty
        self._log_cost = log_cost
        self._cost = 0.
        self._count = 0

    @overrides
    def __call__(self,
                 predictions: List[str],
                 gold_targets: List[str] = None) -> torch.Tensor:
        """ Computes cost under oracle and returns the batch cost.

        Arguments:
            predictions {List[str]} -- predictions generated by a rollout.
            gold_targets {List[str]} -- Orignal Sequence.

        """
        oracle_probs_and_seq_probs = self._oracle.compute_sent_probs(predictions)
        costs = []
        j = 0
        for i, prediction in enumerate(predictions):
            gold_len, pred_len = (len(gold_targets[i]), len(predictions[i]))
            # This encourages model to generate sequences which are of equal
            # length as gold sequence.
            brevity_penality = 0 (1 - float(gold_len)/max(pred_len,1))  \
                                    if (gold_len  > pred_len) and  \
                                        self._add_brevity_penalty \
                            else 0
            if self._log_cost:
                cost = -1 * np.log(oracle_probs_and_seq_probs[i][0]) - brevity_penality
            else:
                cost = 1 - np.exp(brevity_penality) * oracle_probs_and_seq_probs[i][0]
            self._cost += cost
            self._count += 1
            costs.append(cost)

        # We return neg log prob.
        # The objective should be minimize this cost to 0.
        return torch.FloatTensor(costs) \
                    .to(torch.cuda.current_device()
                            if torch.cuda.is_available() else 'cpu')
    @overrides
    def get_metric(self, reset: bool = False):
        """
        Returns
        -------
        The average of all values that were passed to ``__call__``.
        """
        avg_cost = self._cost/self._count

        if reset:
            self.reset()

        return {
            "Cost": avg_cost, 
        }

    @overrides
    def reset(self):
        self._cost = 0
        self._count = 0